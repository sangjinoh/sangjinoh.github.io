---
layout: single
title: "OpenAI Sora: Video generation models as world simulators"
permalink: /studies/paper/Sora
tags: [Paper, LVLM]
categories:
  - 📄 paper
date: 2024-02-16
published: false
---
*본 기술 문서는 비디오 데이터에 대한 대규모 생성 모델 학습을 탐구한다. 구체적으로 다양한 기간, 해상도 및 종횡비의 비디오 및 이미지에 대해 텍스트 조건부 확산 모델을 공동으로 학습한다. 본 연구는 비디오 및 이미지 잠재 코드의 시공간 패치에서 작동하는 변환기 아키텍처를 활용한다. 가장 큰 모델인 Sora는 고해상도 비디오의 1분을 생성할 수 있다. 본 연구의 결과는 비디오 생성 모델의 확장이 물리적 세계의 일반 목적 시뮬레이터를 구축하는 유망한 경로라는 것을 제안한다.*

## 📋 Table of Contents

- [Turning visual data into patches](#turning-visual-data-into-patches)
- [Video compression network](#video-compression-network)
- [Spacetime latent patches](#spacetime-latent-patches)
- [Scaling transformers for video generation](#scaling-transformers-for-video-generation)
- [Variable durations, resolutions, aspect ratios](#variable-durations-resolutions-aspect-ratios)
- [Language understanding](#language-understanding)
- [Prompting with images and videos](#prompting-with-images-and-videos)
- [Image generation capabilities](#image-generation-capabilities)
- [Emerging simulation capabilities](#emerging-simulation-capabilities)
- [Discussion](#discussion)

## Turning visual data into patches
인터넷 규모의 데이터에 대한 학습을 통해 일반적인 능력을 습득하는 대규모 언어 모델에서 영감을 받음.
텍스트의 다양한 모달리티(코드, 수학, 다양한 자연 언어 등)를 우아하게 통합하는 토큰 사용의 성공적인 사례를 참고.
LLM은 텍스트 토큰을 사용하는 반면, Sora는 시각적 패치를 사용함.
패치가 시각적 데이터 모델에 대한 효과적인 표현으로 이전에 입증된 바 있음.
패치를 다양한 유형의 비디오와 이미지에 대한 생성 모델 학습을 위한 매우 확장 가능하고 효과적인 표현으로 발견함.
비디오를 패치로 변환하기 위해 비디오를 낮은 차원의 잠재 공간으로 먼저 압축한 후, 시공간 패치로 표현을 분해함.

## Video compression network
시각적 데이터의 차원을 줄이기 위한 네트워크 학습.
원시 비디오를 입력으로 받음.
시간적으로도 공간적으로도 압축된 잠재 표현을 출력.
Sora는 이 압축된 잠재 공간 내에서 학습됨.
Sora는 이후에 이 압축된 잠재 공간 내에서 비디오를 생성.
생성된 잠재 변수를 다시 픽셀 공간으로 매핑하는 디코더 모델 학습.

## Spacetime latent patches
압축된 입력 비디오로부터 시공간 패치의 순서를 추출.
이 패치들은 변환기 토큰으로 작용함.
이미지에 대해서도 동일하게 적용되며, 이미지는 단일 프레임의 비디오로 간주됨.
패치 기반 표현을 통해 Sora는 다양한 해상도, 기간, 종횡비를 가진 비디오와 이미지에 대해 학습 가능.
추론 시, 무작위로 초기화된 패치를 적절한 크기의 그리드에 배치함으로써 생성된 비디오의 크기를 제어할 수 있음.

## Scaling transformers for video generation
Sora는 주어진 입력 소음 패치와 조건부 정보(예: 텍스트 프롬프트)를 기반으로 원본 "깨끗한" 패치를 예측하도록 학습된 확산 모델입니다.
Sora는 확산 변환기로, 변환기는 언어 모델링, 컴퓨터 비전, 이미지 생성 등 다양한 도메인에서 뛰어난 확장 성능을 보여주었습니다.
확산 변환기는 비디오 모델로서도 효과적으로 확장될 수 있음이 확인되었습니다.
학습이 진행됨에 따라 고정된 시드와 입력을 가진 비디오 샘플의 품질이 현저하게 개선되는 것이 관찰되었습니다.
샘플 품질은 학습에 사용되는 컴퓨트의 양이 증가함에 따라 명확하게 향상됩니다.

## Variable durations, resolutions, aspect ratios
과거에는 비디오를 표준 크기(예: 256x256 해상도의 4초 비디오)로 조정, 자르기, 트리밍하는 방식이 일반적이었음.

원래 크기의 데이터에 대한 학습이 여러 가지 이점을 제공한다는 것을 발견.
### Sampling flexibility
Sora는 와이드스크린 1920x1080p 비디오, 세로 1080x1920 비디오 및 그 사이의 모든 것을 샘플링할 수 있음.
다양한 기기의 원래 종횡비에서 직접 콘텐츠 생성 가능.
전체 해상도에서 생성하기 전에 낮은 크기에서 콘텐츠를 빠르게 프로토타입할 수 있게 함.

### Improved framing and composition
원래 종횡비에서 비디오에 대한 학습이 구성과 구도를 개선한다는 것을 경험적으로 발견.
정사각형으로 자른 훈련 비디오를 사용하는 일반적인 관행과 비교했을 때, Sora는 개선된 구도의 비디오를 생성함.
정사각형 크롭으로 학습된 모델은 주제가 부분적으로만 보이는 비디오를 생성하는 반면, Sora에서의 비디오는 구도가 개선됨.

## Language understanding
대량의 비디오와 해당하는 텍스트 캡션 필요

텍스트-비디오 생성 시스템 학습을 위해, 해당하는 텍스트 캡션과 함께하는 대량의 비디오가 필요함.
재캡션 기법 적용

DALL·E 3에서 소개된 재캡션 기법을 비디오에 적용함.
매우 서술적인 캡셔너 모델을 먼저 학습시키고, 학습 세트의 모든 비디오에 대한 텍스트 캡션 생성에 사용함.
서술적 비디오 캡션의 효과

매우 서술적인 비디오 캡션에 대한 학습이 텍스트의 충실도와 비디오의 전체적인 품질을 향상시킴.
GPT를 이용한 프롬프트 확장

DALL·E 3과 유사하게, 짧은 사용자 프롬프트를 긴 자세한 캡션으로 변환하기 위해 GPT를 활용함.
이를 통해 Sora는 사용자 프롬프트를 정확하게 따르는 고품질의 비디오를 생성할 수 있음.

## Prompting with images and videos
기존 이미지나 비디오를 포함한 다양한 입력으로 Sora 프롬프트 가능.
완벽하게 반복되는 비디오 생성, 정지 이미지 애니메이션, 비디오 시간적 확장 등 다양한 편집 작업 수행 가능.

### Animating DALL·E images
이미지와 프롬프트를 입력으로 받아 비디오 생성 기능 지원.
DALL·E 231 및 DALL·E 330 이미지 기반 비디오 예시 제공.

### Extending generated videos
비디오를 시간적으로 앞이나 뒤로 확장하는 기능.
모든 비디오가 다른 시작점에서 시작하더라도 같은 결말로 이어지는 방식으로 확장 가능.
비디오를 앞뒤로 확장하여 끊김 없는 무한 루프 생성 가능.

### Video-to-video editing
확산 모델을 활용한 비디오 스타일 및 환경 변환 기능(SDEdit 적용).
제로샷으로 입력 비디오의 스타일과 환경을 변환 가능.

### Connecting videos
두 입력 비디오 사이를 점진적으로 보간하여 끊김 없는 전환 생성.
완전히 다른 주제와 장면 구성을 가진 비디오 사이의 부드러운 전환 가능.

## Image generation capabilities
이미지 생성 가능: Sora는 이미지를 생성할 수 있는 기능을 가지고 있음.
가우시안 노이즈 패치 사용: 이미지 생성을 위해 가우시안 노이즈의 패치를 시간적 범위가 한 프레임인 공간 그리드에 배치.
다양한 크기의 이미지 생성: 모델은 다양한 크기의 이미지를 생성할 수 있음.
최대 해상도: 생성 가능한 이미지의 최대 해상도는 2048x2048.

## Emerging simulation capabilities
대규모로 학습될 때, 비디오 모델이 여러 흥미로운 신흥 능력을 보여주는 것을 발견했습니다. 이러한 능력은 Sora가 물리적 세계의 사람들, 동물들 및 환경의 일부 측면을 시뮬레이션할 수 있게 합니다. 이러한 특성은 3D, 객체 등에 대한 명시적인 유도 편향 없이도 나타나며—그것들은 순전히 규모의 현상입니다.
- **3D consistency.**
   - 동적 카메라 모션을 포함하는 비디오 생성 가능.
카메라 이동 및 회전 시 사람 및 장면 요소가 3차원 공간을 통해 일관되게 이동.
- **Long-range coherence and object permanence.**
   - 긴 비디오 샘플링 시 시간적 일관성 유지에 있어 뛰어난 성능.
가려지거나 프레임을 벗어난 객체를 지속시키며, 동일 캐릭터의 여러 샷을 비디오 전체에 걸쳐 일관되게 생성.
- **Interacting with the world.**
   - 세계 상태에 영향을 미치는 간단한 행동 시뮬레이션 가능 (예: 화가의 캔버스에 남기는 획, 햄버거를 먹는 남자의 물린 자국).
- **Simulating digital worlds.**
   - 인공 프로세스 및 비디오 게임 시뮬레이션 가능 (예: Minecraft에서 플레이어 제어 및 세계 렌더링).
"Minecraft" 언급 등의 프롬프트를 통해 제로샷으로 해당 능력 유도 가능.

## Discussion
현재 한계

기본 상호작용의 물리학 (예: 유리 깨짐) 정확하게 모델링하지 못함.
객체 상태의 올바른 변화를 항상 초래하지 않는 상호작용 (예: 음식 먹기).
긴 기간 샘플에서 발생하는 비일관성, 객체의 갑작스러운 출현 등의 실패 모드 존재.
미래 가능성

비디오 모델의 지속적인 확장은 물리적 및 디지털 세계와 그 안에 사는 객체, 동물, 사람들을 시뮬레이션할 수 있는 능력 있는 시뮬레이터 개발로 가는 유망한 경로로 보임.
Sora가 현재 가지고 있는 능력은 이 방향으로의 발전 가능성을 시사함.
---
layout: single
title: "What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis"
permalink: /studies/paper/Deep Text Recognition
tags: [Paper, Vision AI]
categories:
  - 📄 paper
use_math: true
date: 2023-10-11
published: false
---
*최근 몇 년 동안 Scene Text Recognition(STR) 모델에 대한 많은 새로운 제안이 도입되었다. 기존 연구들은 기술의 경계를 넓혔다고 주장하지만, 학습 및 평가 데이터셋의 일관성 없는 선택으로 인해 전체적이고 공정한 비교가 대부분 누락되었다. 본 논문은 세 가지 주요 기여를 통해 이 어려움을 해결한다. 첫째, 학습 및 평가 데이터셋의 불일치와 불일치로 인한 성능 격차를 검토한다. 둘째, 대부분의 기존 STR 모델에 맞춰질 수 있는 통합된 네 단계 STR 프레임워크를 소개한다. 이 프레임워크를 사용하면 이전에 제안된 STR 모듈의 광범위한 평가와 이전에 탐색되지 않은 모듈 조합의 발견이 가능하다. 셋째, 하나의 일관된 학습 및 평가 데이터셋 세트에서 정확도, 속도 및 메모리 요구 사항 측면에서 모듈별 기여도를 분석한다. 이러한 분석은 현재의 비교에서 이해하기 어려운 기존 모듈의 성능 향상에 대한 장애를 해결한다. 본 논문의 코드는 [이 링크](https://github.com/clovaai/deep-text-recognition-benchmark){:target="_blank"}에서 공개적으로 이용 가능하다.*

## 📋 Table of Contents

- [1. Introduction](#1-introduction)
- [2. Dataset Matters in STR](#2-dataset-matters-in-str)
- [3. STR Framework Analysis](#3-str-framework-analysis)
- [4. Experiment and Analysis](#4-experiment-and-analysis)
- [5. Conclusion](#5-conclusion)

## 1. Introduction
- 자연 장면에서 텍스트를 읽는 장면 텍스트 인식(STR)은 다양한 산업 응용 분야에서 중요한 작업입니다.
- 기존 OCR 방법들은 실제 세계의 다양한 텍스트 외형과 장면이 캡처된 불완전한 조건들로 인해 STR 작업에서 그만큼 효과적이지 못했습니다.
- 이전 연구들은 특정 도전을 해결하는 깊은 신경망으로 이루어진 다단계 파이프라인을 제안했습니다.
- 새롭게 제안된 모듈이 현재의 예술에 비해 어떻게 개선되었는지 평가하기 어려웠습니다. 이는 다른 평가 및 테스트 환경으로 인한 것입니다.
- 학습 데이터셋과 평가 데이터셋 사용에 일관성이 없어 다른 모델들 사이의 성능 비교가 공정하지 않았습니다.
- 이 논문은 STR 데이터셋 사용의 일관성 없음과 그 원인을 분석합니다.
- 통합된 STR 프레임워크를 소개하여 기존 방법에 대한 공통적인 관점을 제공합니다.
- 통합 실험 설정 아래에서 정확도, 속도, 및 메모리 요구 사항 측면에서 모듈별 기여를 연구합니다.
- 개별 모듈의 기여를 더 엄격하게 평가하고, 현재의 예술보다 개선된 이전에 간과된 모듈 조합을 제안합니다.
- 벤치마크 데이터셋에서의 실패 사례를 분석하여 STR에서 남아있는 도전 과제를 식별합니다.

## 2. Dataset Matters in STR
- 이전 작업에서 사용된 다양한 학습 및 평가 데이터셋 검토
- 데이터셋 사용에서의 차이점 다룸
- 각 작업이 데이터셋을 구성하고 사용하는 방식의 차이 강조
- 다른 작업 간 성능 비교 시 일관성 없음으로 인한 편향 조사
- 데이터셋의 일관성 없음으로 인한 성능 격차 측정 및 논의

### 2.1. Synthetic datasets for training
- STR 모델 학습 시 실제 데이터 대신 합성 데이터셋 사용
- 가장 인기 있는 두 가지 합성 데이터셋 소개:
  - MJSynth (MJ): STR을 위해 설계된 합성 데이터셋, 8.9M 단어 상자 이미지 포함. 생성 과정에는 폰트 렌더링, 테두리 및 그림자 렌더링, 배경 색칠, 폰트/테두리/배경의 합성, 투영 왜곡 적용, 실제 세계 이미지와의 혼합, 노이즈 추가가 포함됨.
  - SynthText (ST): 장면 텍스트 검출을 위해 원래 설계된 또 다른 합성 생성 데이터셋. 단어 상자를 자르고 비알파벳 문자를 필터링하여 STR에도 사용됨. 5.5M 학습 데이터가 있음.
- 이전 작업들은 MJ, ST, 또는 다른 출처들의 다양한 조합을 사용
- 데이터셋 사용의 일관성 부족이 성능 개선이 모듈의 기여 때문인지, 아니면 더 나은 또는 더 큰 학습 데이터 때문인지 의문을 제기함
- 미래의 STR 연구에서 사용된 학습 데이터셋을 명확히 표시하고 모델을 동일한 학습 세트를 사용하여 비교할 것을 제안

### 2.2. Real-world datasets for evaluation
- STR 모델 평가에 널리 사용되는 일곱 가지 실제 데이터셋
- 데이터셋의 부분집합 사용으로 인한 일관성 없는 비교 문제
- 데이터셋을 "정규"와 "비정규"로 분류
- 정규 데이터셋은 비교적 쉬운 STR 사례를 대표:
  - IIIT5K-Words (IIIT): Google 이미지 검색에서 크롤링, 2,000개 학습 이미지와 3,000개 평가 이미지
  - Street View Text (SVT): Google Street View에서 수집된 야외 거리 이미지, 257개 학습 이미지와 647개 평가 이미지
  - ICDAR2003 (IC03): ICDAR 2003 대회용, 1,156개 학습 이미지와 1,110개 평가 이미지 (비알파벳 문자 또는 3자 미만 단어 제외 시 867개)
  - ICDAR2013 (IC13): ICDAR 2013 대회용, 대부분 IC03 이미지 상속, 848개 학습 이미지와 1,095개 평가 이미지 (비알파벳 문자를 포함한 단어 제외 시 1,015개)
- 평가를 위해 데이터셋의 다른 버전 사용으로 인한 차이점 존재

비정규 데이터셋은 STR에 대한 더 어려운 코너 케이스를 포함하며, 주로 곡선이 있거나 임의로 회전하거나 왜곡된 텍스트를 포함합니다:

ICDAR2015 (IC15)

목적: ICDAR 2015 강건한 읽기 대회를 위해 제작됨.
데이터셋 특징: Google Glass를 통해 캡처된 4,468개의 학습 이미지와 2,077개의 평가 이미지 포함. 많은 이미지가 노이즈가 많고, 흐리며, 회전되었고, 일부는 해상도가 낮음.
평가 버전: 연구자들은 1,811개와 2,077개 이미지의 두 가지 다른 버전으로 평가했음. 비알파벳 문자 이미지와 일부 극도로 회전된, 관점이 변경된, 곡선이 있는 이미지를 제외하고 1,811개의 이미지만 사용함.
SVT Perspective (SP)

데이터셋 특징: Google Street View에서 수집된 645개의 평가 이미지 포함. 많은 이미지가 비정면 시점으로 인한 관점 투영을 포함함.
CUTE80 (CT)

데이터셋 특징: 자연 장면에서 수집된 288개의 잘린 이미지로 평가됨. 많은 이미지가 곡선 텍스트 이미지를 포함함.
표 1은 이전 작업들이 다른 벤치마크 데이터셋에서 모델을 평가했다는 중요한 문제를 강조합니다. 특히, IC03, IC13, IC15의 다른 버전에서 평가가 이루어졌으며, IC03에서는 7개의 예시가 이전 성능들과 비교할 때 0.8%라는 큰 성능 격차를 일으킬 수 있습니다. IC13과 IC15에서는 예시 번호의 격차가 IC03보다 더 큽니다.

## 3. STR Framework Analysis
- 장면 텍스트 인식(STR) 프레임워크는 네 단계로 구성되어 있으며, 각 단계는 다음과 같습니다:
  - 변환(Transformation): Spatial Transformer Network(STN)를 사용하여 입력 텍스트 이미지를 정규화하여 후속 단계를 용이하게 합니다.
  - 특성 추출(Feature extraction): 입력 이미지를 문자 인식에 관련된 속성에 초점을 맞춘 표현으로 매핑하면서 글꼴, 색상, 크기, 배경 등 관련 없는 특성을 억제합니다.
  - 시퀀스 모델링(Sequence modeling): 문자의 시퀀스 내에서 맥락 정보를 포착하여 다음 단계에서 각 문자를 더 견고하게 예측할 수 있게 합니다.
  - 예측(Prediction): 이미지의 식별된 특성으로부터 출력 문자 시퀀스를 추정합니다.

### 3.1. Transformation stage
- 이 단계의 모듈은 입력 이미지 �X를 정규화된 이미지 �~X~ 로 변환합니다.
- 자연 장면에서의 텍스트 이미지는 다양한 형태를 띠며, 곡선이 있거나 기울어진 텍스트를 포함할 수 있습니다.
- 입력 이미지가 변경 없이 제공되면, 후속 특성 추출 단계는 이러한 기하학적 변형에 대해 불변하는 표현을 학습해야 합니다.
- 이러한 부담을 줄이기 위해, 씬 플레이트 스플라인(TPS) 변환을 적용합니다. TPS는 공간 변환 네트워크(STN)의 변형으로, 텍스트 라인의 다양한 종횡비에 유연하게 적용될 수 있습니다.
- TPS는 일련의 기준점 사이에서 부드러운 스플라인 보간을 사용하며, 상단과 하단을 둘러싼 점들에서 다수의 기준점을 찾아 문자 영역을 미리 정의된 직사각형으로 정규화합니다.
- 프레임워크는 TPS의 선택 또는 선택 해제를 허용합니다.

### 3.2. Feature extraction stage
- 특성 추출 단계에서 연구된 세 가지 아키텍처는 다음과 같습니다:
  - VGG - 여러 개의 컨볼루셔널 레이어를 거친 후 몇 개의 완전 연결 레이어로 구성됩니다.
  - RCNN - 문자 형태에 따라 수용 필드를 조정할 수 있도록 재귀적으로 적용될 수 있는 CNN의 변형입니다.
  - ResNet - 상대적으로 더 깊은 CNN의 학습을 용이하게 하는 잔류 연결을 가진 CNN입니다.

### 3.3. Sequence modeling stage
- 특성 재구성: 특성 추출 단계에서 추출된 특성들은 시퀀스 �V로 재구성됩니다. 특성 맵의 각 열 ��v i​ 는 시퀀스의 프레임으로 사용됩니다.
- 문맥 정보 부족: 이 시퀀스는 문맥 정보가 부족할 수 있습니다.
- 양방향 LSTM (BiLSTM) 사용: 일부 이전 작업들은 더 나은 시퀀스 �=Seq:(�) H=Seq:(V)를 만들기 위해 양방향 LSTM(BiLSTM)을 사용합니다.
- BiLSTM 제거 옵션: Rosetta와 같은 일부 방법은 계산 복잡성과 메모리 소비를 줄이기 위해 BiLSTM을 제거했습니다.
- BiLSTM 선택 가능: 프레임워크는 BiLSTM의 선택 또는 선택 해제를 허용합니다.

### 3.4. Prediction stage
- 이 단계에서는 입력 �H로부터 문자 시퀀스 �=�1,�2,…Y=y 1​ ,y ​ ,…를 예측하는 두 가지 주요 방법이 있습니다:
  - 연결주의 시간 분류 (CTC)
    - 고정된 수의 특성이 주어지더라도 고정되지 않은 수의 시퀀스를 예측할 수 있게 합니다.
    - 각 열 ℎ�∈�h i​ ∈H에서 문자를 예측하고, 반복되는 문자와 공백을 삭제함으로써 전체 문자 시퀀스를 고정되지 않은 문자 스트림으로 수정하는 방법을 사용합니다.
  - 주의 기반 시퀀스 예측 (Attention-based Sequence Prediction)
    - 입력 시퀀스 내의 정보 흐름을 자동으로 포착하여 출력 시퀀스를 예측합니다.
    - STR 모델이 출력 클래스 의존성을 나타내는 문자 수준 언어 모델을 학습할 수 있게 합니다.

## 4. Experiment and Analysis
### 4.1. Implementation detail
- 학습 및 평가 데이터셋의 중요성: 학습, 검증, 평가 데이터셋의 선택은 STR 모델의 성능 측정에 큰 영향을 미칩니다.
- STR 학습 및 모델 선택:
  - 학습 데이터: MJSynth 8.9M과 SynthText 5.5M(총 14.4M).
  - 옵티마이저: AdaDelta, 감쇠율 0.95.
  - 학습 배치 크기: 192, 반복 횟수: 300K.
  - 경사 클리핑: 크기 5에서 사용.
  - 파라미터 초기화: He의 방법.
  - 검증 데이터: IC13, IC15, IIIT, SVT의 학습 세트 합집합.
  - 모델 검증: 2000 학습 스텝마다 실행, 가장 높은 정확도 모델 선택.
  - IC03 데이터는 평가 데이터셋과의 중복을 피하기 위해 제외.
- 평가 메트릭:
  - 정확도: 9개 실세계 평가 데이터셋 및 통합 평가 데이터셋(총 8,539개 이미지)에서 이미지 당 단어 예측 성공률 측정.
  - 속도: 주어진 텍스트를 인식하는 데 필요한 평균 클럭 시간(밀리초 단위) 측정.
  - 메모리: 전체 STR 파이프라인에서 학습 가능한 부동 소수점 파라미터 수 계산.
- 실험 환경:
  - Intel Xeon(R) E5-2630 v4 2.20GHz CPU, NVIDIA TESLA P40 GPU, 252GB RAM.
  - 모든 실험은 NAVER Smart Machine Learning (NSML) 플랫폼에서 수행됨.

### 4.2. Analysis on training datasets
- 학습 데이터셋의 선택이 성능에 미치는 영향: 다양한 그룹의 학습 데이터셋 사용이 벤치마크에서의 성능에 큰 영향을 미칩니다.
- 학습 데이터셋에 대한 실험 결과:
  - MJSynth만 사용했을 때: 80.0% 총 정확도.
  - SynthText만 사용했을 때: 75.6% 정확도.
  - MJSynth와 SynthText 둘 다 사용했을 때: 84.1% 정확도.
- 데이터셋의 결합이 정확도 향상에 기여: MJSynth와 SynthText의 결합은 개별 사용보다 4.1% 이상 정확도를 향상시킵니다.
- 다양한 학습 데이터셋 사용의 복잡성 인식: 다른 학습 데이터셋 사용 결과의 성능 비교는 불가능하며, 모델의 기여를 입증하기 어렵습니다.
- 학습 데이터의 다양성의 중요성: MJSynth의 20%와 SynthText의 20%를 함께 학습시킨 결과, 개별 사용보다 높은 정확도(81.3%)를 달성하며, 학습 데이터의 다양성이 학습 예제의 수보다 중요할 수 있음을 시사합니다.

### 4.3. Analysis of trade-offs for module combinations
- 정확도-속도 트레이드오프
  - T1부터 T5까지의 모듈 조합은 순차적으로 다음 모듈을 도입함으로써 정확도를 향상시킵니다: ResNet, BiLSTM, TPS, Attn.
  - T1은 변환 또는 순차 모듈을 포함하지 않아 최소 시간을 소요합니다.
  - T5까지 각 단계마다 단일 모듈이 변경되어, 계산 효율성의 비용으로 성능이 향상됩니다.
  - ResNet, BiLSTM, TPS는 비교적 적당한 전체 속도 저하로 정확도를 크게 향상시킵니다.
  - Attn 모듈의 추가는 효율성의 큰 비용으로 정확도를 단 1.1%만 향상시킵니다.
- 정확도-메모리 트레이드오프
  - P1부터 P5까지의 모듈 조합은 메모리와 정확도 사이의 트레이드오프를 보여줍니다.
  - P1은 메모리 소비가 가장 적은 모델입니다.
  - P1에서 P5로 가면서 변경된 모듈은 Attn, TPS, BiLSTM, ResNet입니다.
  - RCNN은 VGG와 비교하여 더 가볍고 좋은 정확도-메모리 트레이드오프를 제공합니다.
  - 변환, 순차, 예측 모듈은 메모리 소비에 크게 기여하지 않으며, 정확도 향상을 제공합니다.
  - ResNet의 추가는 메모리 소비를 크게 증가시키면서 정확도를 약간 향상시킵니다.
- 속도와 메모리에 가장 중요한 모듈
  - 속도에 대해 Attn 모듈의 추가가 전체 STR 모델을 상당히 느리게 합니다.
  - 메모리에 대해 특징 추출기가 가장 중요한 기여를 합니다.
  - 이 분석을 통해, 다른 응용 시나리오와 제약 조건 하에 있는 사용자는 자신의 필요에 맞는 최적의 트레이드오프를 위해 다양한 모듈 조합을 고려할 수 있습니다.

### 4.4. Module analysis
- 모듈별 평균화된 정확도 계산: 테이블 2에 포함된 모듈 조합을 평균내어 각 모듈의 성능을 평가합니다.
- 성능 향상 대 자원 요구사항: 각 단계에서 모듈을 업그레이드하면 추가적인 자원, 시간, 또는 메모리가 필요하지만, 이는 성능 향상을 제공합니다.
- 비정규 데이터셋에서의 성능 향상: 비정규 데이터셋에서의 성능 향상이 정규 벤치마크의 약 두 배임을 발견했습니다.
- 정확도 향상 대비 시간 사용 최적 순서: None-VGG-None-CTC의 기본 조합에서 ResNet, BiLSTM, TPS, Attn 순으로 모듈을 업그레이드하는 것이 시간 대비 정확도 향상에 가장 효율적입니다.
- 정확도-메모리 관점에서의 최적 순서: RCNN, Attn, TPS, BiLSTM, ResNet 순으로 모듈을 업그레이드하는 것이 메모리 대비 정확도 향상에 가장 효율적입니다.
- 시간 대 메모리에서의 모듈 순서 차이: 시간에 대한 모듈의 효율적 순서는 메모리에 대한 순서와 반대입니다.
- 모듈별 기여도 분석:
  - TPS 변환: 곡선 및 원근 텍스트를 표준화된 뷰로 정규화하여 텍스트 인식을 개선합니다.
  - ResNet: 밀집한 배경 잡음과 보지 못한 폰트에서 더 나은 표현력을 제공합니다.
  - BiLSTM: 더 나은 컨텍스트 모델링을 통해 관련 없이 자른 문자를 무시할 수 있습니다.
  - Attn (주의 메커니즘): 누락되거나 가려진 문자를 찾아내어 텍스트 인식을 향상시킵니다.

### 4.5. Failure case analysis
- 캘리그래피 폰트: 브랜드 또는 상점 이름에 사용된 독특한 폰트 스타일은 일반화된 시각적 특성을 제공하는 새로운 특징 추출기가 필요합니다.
- 세로 텍스트: 현재의 STR 모델들은 주로 수평 텍스트 이미지를 처리하도록 설계되어 있어, 세로 텍스트를 효과적으로 처리할 수 없습니다.
- 특수 문자: 현재 벤치마크는 특수 문자를 평가하지 않으므로, 특수 문자를 학습에서 제외하게 되어 이를 영숫자 문자로 잘못 인식하게 됩니다.
- 심각한 가림 현상: 현재 방법들은 문맥 정보를 충분히 활용하지 못해 객체가 가려진 경우를 효과적으로 처리하지 못합니다.
- 저해상도: 저해상도 이미지를 효과적으로 처리하기 위한 명시적인 방법이 부족합니다. 이미지 피라미드나 초해상도 모듈이 해결책이 될 수 있습니다.
- 라벨 노이즈: 실패 예제 중 일부는 잘못된 라벨링 때문이었습니다. 특수 문자를 고려하지 않을 때의 오류 라벨링 비율은 1.3%, 특수 문자를 고려할 때는 6.1%, 대소문자를 고려할 때는 24.1%였습니다.

## 5. Conclusion
- 문제의 배경: 새로운 장면 텍스트 인식(STR) 모델들이 크게 발전했음에도 불구하고, 일관성 없는 벤치마크로 인해 제안된 모듈이 STR 기본 모델을 어떻게 개선하는지 판단하기 어려웠습니다.
- 연구의 목표: 이전에 일관되지 않은 실험 설정으로 인해 가려졌던 기존 STR 모델의 기여를 분석합니다.
- 연구 방법: 주요 STR 방법론들 간의 공통 프레임워크 및 일곱 개의 벤치마크 평가 데이터셋과 두 개의 학습 데이터셋(MJ와 ST) 도입.
- 제공된 비교: 주요 STR 방법론들 간의 공정한 비교를 제공하며, 어떤 모듈이 가장 큰 정확도, 속도, 그리고 크기의 이득을 가져오는지 분석.
- 분석과 기여: STR의 전형적인 도전 과제와 남아 있는 실패 사례에 대해 모듈별 기여에 대한 광범위한 분석을 제공함.